{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NIk6cgTfluF3",
        "ozbhab6Wn_uJ",
        "g8WB8kbsasz2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIk6cgTfluF3"
      },
      "source": [
        "#Data scraping and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2om5xbwl28S"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "from selenium import webdriver\r\n",
        "import time\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPjca_i1l7wr"
      },
      "source": [
        "driver = webdriver.Chrome('add your local path to chromewebdriver.exe')\r\n",
        "driver.get('https://www.simplyscripts.com/genre/romance-scripts.html')\r\n",
        "time.sleep(5)\r\n",
        "doc = driver.page_source\r\n",
        "counter = 0\r\n",
        "counter1 = 0\r\n",
        "soup = BeautifulSoup(doc, features='html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG8xP9mQl_Ni"
      },
      "source": [
        "for link in soup.find_all('a'):\r\n",
        "    counter += 1\r\n",
        "    lnk = link.get('href')\r\n",
        "    # Check whether the link has a text file, returns a 200 code and is not of a defunct website\r\n",
        "    if(lnk.endswith('txt') and requests.get(lnk).status_code == 200 and 'weeklyscript.com' not in lnk and 'angelfire.com' not in lnk):        \r\n",
        "        counter1 += 1\r\n",
        "        driver.get(lnk)\r\n",
        "        doc1 = driver.page_source\r\n",
        "        soup1 = BeautifulSoup(doc1, features='html.parser')\r\n",
        "        with open('traindata1.txt', 'a') as f:\r\n",
        "            for txt in soup1.find_all('pre'):\r\n",
        "                pattern = r'<pre style=\"word-wrap: break-word; white-space: pre-wrap;\">'\r\n",
        "                txt = re.sub(pattern, '', str(txt))\r\n",
        "                pattern = r'</pre>'\r\n",
        "                txt = re.sub(pattern, '', str(txt))\r\n",
        "                f.write(txt)\r\n",
        "    else:\r\n",
        "        print(str(counter) + \". No text files found!\")\r\n",
        "\r\n",
        "print(\"Total scripts saved:\", counter1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZutKSYKgecB"
      },
      "source": [
        "# Markov Model (some code is courtesy of @ashwinmj)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FND9fvtgecD"
      },
      "source": [
        "import string\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOx1useZgecE"
      },
      "source": [
        "# Path of the text file containing the training data\n",
        "data = 'traindata1.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYrqyZssgecE"
      },
      "source": [
        "#Helper functions\n",
        "def remPunc(sentence):\n",
        "    return sentence.translate(str.maketrans('','', string.punctuation))\n",
        "\n",
        "def dictAdd(dictionary, key, value):\n",
        "    if key not in dictionary:\n",
        "        dictionary[key] = []\n",
        "    dictionary[key].append(value)\n",
        "\n",
        "def probDictAdd(lst):\n",
        "    probs = {}\n",
        "    lstLen = len(lst)\n",
        "    for item in lst:\n",
        "        probs[item] = probs.get(item, 0) + 1\n",
        "    for key, value in probs.items():\n",
        "        probs[key] = value / lstLen\n",
        "    return probs\n",
        "\n",
        "def randomWord(dictionary):\n",
        "    p0 = np.random.random()\n",
        "    cumulative = 0\n",
        "    for key, value in dictionary.items():\n",
        "        cumulative += value\n",
        "        if p0 < cumulative:\n",
        "            return key\n",
        "    assert(False)\n",
        "\n",
        "firstWord = {}\n",
        "secondWord = {}\n",
        "transitions = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf49lwJPgecG"
      },
      "source": [
        "# Trains a Markov model based on the data in data\n",
        "def train():\n",
        "    for line in open(data, encoding='latin-1'):\n",
        "        tokens = remPunc(line.rstrip().lower()).split()\n",
        "        tokensLen = len(tokens)\n",
        "        for i in range(tokensLen):\n",
        "            token = tokens[i]\n",
        "            if i == 0:\n",
        "                firstWord[token] = firstWord.get(token, 0) + 1\n",
        "            else:\n",
        "                prev = tokens[i - 1]\n",
        "                if i == tokensLen - 1:\n",
        "                    dictAdd(transitions, (prev, token), 'END')\n",
        "                if i == 1:\n",
        "                    dictAdd(secondWord, prev, token)\n",
        "                else:\n",
        "                    prevPrev = tokens[i - 2]\n",
        "                    dictAdd(transitions, (prevPrev, prev), token)\n",
        "    \n",
        "    # Normalize the distributions\n",
        "    total = sum(firstWord.values())\n",
        "    for key, value in firstWord.items():\n",
        "        firstWord[key] = value / total\n",
        "        \n",
        "    for prevWord, nextWordLst in secondWord.items():\n",
        "        secondWord[prevWord] = probDictAdd(nextWordLst)\n",
        "        \n",
        "    for pair, nextWordLst in transitions.items():\n",
        "        transitions[pair] = probDictAdd(nextWordLst)\n",
        "    \n",
        "    print('Training successful.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drjuyQLzgecH"
      },
      "source": [
        "numPred = 15\n",
        "\n",
        "# Function to autocomplete sample text\n",
        "def autocomplete(w0, w1):\n",
        "    for i in range(numPred):\n",
        "        sentence = []\n",
        "        # Initial word\n",
        "        word0 = w0\n",
        "        sentence.append(word0)\n",
        "        # Second word\n",
        "        word1 = w1\n",
        "        sentence.append(word1)\n",
        "        # Subsequent words untill END\n",
        "        endc = 0\n",
        "        while True:\n",
        "            if word1 == 'END':\n",
        "              break\n",
        "            word2 = randomWord(transitions[(word0, word1)])\n",
        "            while word2 == 'END':\n",
        "                endc+=1\n",
        "                if endc >= 100:\n",
        "                  break\n",
        "                else:\n",
        "                  word2 = randomWord(transitions[(word0, word1)])\n",
        "            sentence.append(word2)\n",
        "            word0 = word1\n",
        "            word1 = word2\n",
        "        print(' '.join(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biL1EevtgecH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befb4cc8-9f5c-45bf-882a-dc1682595119"
      },
      "source": [
        "np.random.seed(0)\r\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training successful.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iygduYYi-0nq",
        "outputId": "d518d4bd-4df4-4004-f576-13cbecefb081"
      },
      "source": [
        "np.random.seed(10)\r\n",
        "autocomplete('she', 'feels')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "she feels for him for six years ago i barely made it up looks briefly to his door leaving melvin alone in the rear windows exploding outward in a shop being closed other shops END\n",
            "she feels like shes some fantastic new species END\n",
            "she feels a presence at her clothes of all these houses were convents END\n",
            "she feels she takes off the plate of eggs bacon and sausage with END\n",
            "she feels kind of wild huh END\n",
            "she feels like how being in the small size i always say no one helps them when they break apart they stretch themselves corrado END\n",
            "she feels she takes it aboard the marshal starts climbing up towards the jury waits anxiously for her to abort her mission completed she turns and takes him a deep breath he turns to snow where the wavin wheat can sure smell END\n",
            "she feels kind of pers END\n",
            "she feels like how being in love with architecture END\n",
            "she feels separate from him the door opens and the first time its perry the professor coming END\n",
            "she feels kind of person END\n",
            "she feels kind of roar from the market virgil END\n",
            "she feels for him to plead for END\n",
            "she feels kind of doctor END\n",
            "she feels kind of adrift and END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozbhab6Wn_uJ"
      },
      "source": [
        "#Recurrent Neural Network Model v1\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGwR5lZsK5c3"
      },
      "source": [
        "import os \r\n",
        "import tensorflow as tf\r\n",
        "tf.compat.v1.enable_eager_execution()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q53-vfXGO7mZ",
        "outputId": "d3110958-8612-4a44-bc5e-e7c492f880c0"
      },
      "source": [
        "try:\r\n",
        "  device_name = os.environ['COLAB_TPU_ADDR']\r\n",
        "  TPU_ADDRESS = 'grpc://' + device_name\r\n",
        "  print('Found TPU at: {}'.format(TPU_ADDRESS))\r\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\r\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\r\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\n",
        "  tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)\r\n",
        "except KeyError:\r\n",
        " print('TPU not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found TPU at: grpc://10.85.8.10:8470\n",
            "WARNING:tensorflow:TPU system grpc://10.85.8.10:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.85.8.10:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.85.8.10:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.85.8.10:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1OKdVbEn8E0"
      },
      "source": [
        "import sys\r\n",
        "import numpy\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYKRnCqhL-55",
        "outputId": "bab1987b-d0c6-4b3b-f5f9-b1f211c5aa72"
      },
      "source": [
        "# load ascii text and covert to lowercase\r\n",
        "filename = \"traindata1.txt\"\r\n",
        "raw_text = open(filename, 'r', encoding='latin-1').read()\r\n",
        "raw_text = raw_text.lower()\r\n",
        "\r\n",
        "# create mapping of unique chars to integers, and a reverse mapping\r\n",
        "chars = sorted(list(set(raw_text)))\r\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\r\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\r\n",
        "\r\n",
        "# summarize the loaded data\r\n",
        "n_chars = len(raw_text)\r\n",
        "n_vocab = len(chars)\r\n",
        "print(\"Total Characters: \", n_chars)\r\n",
        "print(\"Total Vocab: \", n_vocab)\r\n",
        "\r\n",
        "# prepare the dataset of input to output pairs encoded as integers\r\n",
        "seq_length = 100\r\n",
        "dataX = []\r\n",
        "dataY = []\r\n",
        "for i in range(0, n_chars - seq_length, 1):\r\n",
        "\tseq_in = raw_text[i:i + seq_length]\r\n",
        "\tseq_out = raw_text[i + seq_length]\r\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\r\n",
        "\tdataY.append(char_to_int[seq_out])\r\n",
        "n_patterns = len(dataX)\r\n",
        "print(\"Total Patterns: \", n_patterns)\r\n",
        "\r\n",
        "# reshape X to be [samples, time steps, features]\r\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\r\n",
        "\r\n",
        "# normalize\r\n",
        "X = X / float(n_vocab)\r\n",
        "\r\n",
        "# one hot encode the output variable\r\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  2456084\n",
            "Total Vocab:  77\n",
            "Total Patterns:  2455984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF78DuLCMDSh",
        "outputId": "85112b2a-56a1-434e-f771-5fab3c0daecc"
      },
      "source": [
        "with tpu_strategy.scope():\r\n",
        "  # define the LSTM model\r\n",
        "  model = Sequential()\r\n",
        "  model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\r\n",
        "  model.add(Dropout(0.1))\r\n",
        "  model.add(LSTM(128))\r\n",
        "  model.add(Dropout(0.1))\r\n",
        "  model.add(Dense(y.shape[1], activation='softmax'))\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "\r\n",
        "# define the checkpoint\r\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "# fit the model\r\n",
        "model.fit(X, y, epochs=20, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "38375/38375 [==============================] - 714s 18ms/step - loss: 2.4581\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.27012, saving model to weights-improvement-01-2.2701.hdf5\n",
            "Epoch 2/20\n",
            "38375/38375 [==============================] - 706s 18ms/step - loss: 1.9910\n",
            "\n",
            "Epoch 00002: loss improved from 2.27012 to 1.94571, saving model to weights-improvement-02-1.9457.hdf5\n",
            "Epoch 3/20\n",
            "38375/38375 [==============================] - 699s 18ms/step - loss: 1.8397\n",
            "\n",
            "Epoch 00003: loss improved from 1.94571 to 1.81851, saving model to weights-improvement-03-1.8185.hdf5\n",
            "Epoch 4/20\n",
            "38375/38375 [==============================] - 701s 18ms/step - loss: 1.7598\n",
            "\n",
            "Epoch 00004: loss improved from 1.81851 to 1.74710, saving model to weights-improvement-04-1.7471.hdf5\n",
            "Epoch 5/20\n",
            "38375/38375 [==============================] - 702s 18ms/step - loss: 1.7053\n",
            "\n",
            "Epoch 00005: loss improved from 1.74710 to 1.69879, saving model to weights-improvement-05-1.6988.hdf5\n",
            "Epoch 6/20\n",
            "38375/38375 [==============================] - 703s 18ms/step - loss: 1.6699\n",
            "\n",
            "Epoch 00006: loss improved from 1.69879 to 1.66654, saving model to weights-improvement-06-1.6665.hdf5\n",
            "Epoch 7/20\n",
            "38375/38375 [==============================] - 705s 18ms/step - loss: 1.6425\n",
            "\n",
            "Epoch 00007: loss improved from 1.66654 to 1.63689, saving model to weights-improvement-07-1.6369.hdf5\n",
            "Epoch 8/20\n",
            "38375/38375 [==============================] - 720s 19ms/step - loss: 1.6149\n",
            "\n",
            "Epoch 00008: loss improved from 1.63689 to 1.61245, saving model to weights-improvement-08-1.6125.hdf5\n",
            "Epoch 9/20\n",
            "38375/38375 [==============================] - 715s 19ms/step - loss: 1.5953\n",
            "\n",
            "Epoch 00009: loss improved from 1.61245 to 1.59309, saving model to weights-improvement-09-1.5931.hdf5\n",
            "Epoch 10/20\n",
            "38375/38375 [==============================] - 731s 19ms/step - loss: 1.5778\n",
            "\n",
            "Epoch 00010: loss improved from 1.59309 to 1.57707, saving model to weights-improvement-10-1.5771.hdf5\n",
            "Epoch 11/20\n",
            "38375/38375 [==============================] - 747s 19ms/step - loss: 1.5611\n",
            "\n",
            "Epoch 00011: loss improved from 1.57707 to 1.55958, saving model to weights-improvement-11-1.5596.hdf5\n",
            "Epoch 12/20\n",
            "38375/38375 [==============================] - 742s 19ms/step - loss: 1.5436\n",
            "\n",
            "Epoch 00012: loss improved from 1.55958 to 1.54478, saving model to weights-improvement-12-1.5448.hdf5\n",
            "Epoch 13/20\n",
            "38375/38375 [==============================] - 731s 19ms/step - loss: 1.5342\n",
            "\n",
            "Epoch 00013: loss improved from 1.54478 to 1.53312, saving model to weights-improvement-13-1.5331.hdf5\n",
            "Epoch 14/20\n",
            "38375/38375 [==============================] - 735s 19ms/step - loss: 1.5212\n",
            "\n",
            "Epoch 00014: loss improved from 1.53312 to 1.52088, saving model to weights-improvement-14-1.5209.hdf5\n",
            "Epoch 15/20\n",
            "38375/38375 [==============================] - 738s 19ms/step - loss: 1.5131\n",
            "\n",
            "Epoch 00015: loss improved from 1.52088 to 1.51320, saving model to weights-improvement-15-1.5132.hdf5\n",
            "Epoch 16/20\n",
            "38375/38375 [==============================] - 739s 19ms/step - loss: 1.5026\n",
            "\n",
            "Epoch 00016: loss improved from 1.51320 to 1.50315, saving model to weights-improvement-16-1.5032.hdf5\n",
            "Epoch 17/20\n",
            "38375/38375 [==============================] - 752s 20ms/step - loss: 1.4939\n",
            "\n",
            "Epoch 00017: loss improved from 1.50315 to 1.49448, saving model to weights-improvement-17-1.4945.hdf5\n",
            "Epoch 18/20\n",
            "38375/38375 [==============================] - 748s 19ms/step - loss: 1.4877\n",
            "\n",
            "Epoch 00018: loss improved from 1.49448 to 1.48806, saving model to weights-improvement-18-1.4881.hdf5\n",
            "Epoch 19/20\n",
            "38375/38375 [==============================] - 744s 19ms/step - loss: 1.4790\n",
            "\n",
            "Epoch 00019: loss improved from 1.48806 to 1.47985, saving model to weights-improvement-19-1.4798.hdf5\n",
            "Epoch 20/20\n",
            "38375/38375 [==============================] - 748s 19ms/step - loss: 1.4745\n",
            "\n",
            "Epoch 00020: loss improved from 1.47985 to 1.47349, saving model to weights-improvement-20-1.4735.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd70e2d8320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jy02gkGKaAB",
        "outputId": "e37881d3-e9c5-450a-bee0-bb3020617159"
      },
      "source": [
        "# load the network weights\r\n",
        "filename = \"weights-improvement-20-1.4735.hdf5\"\r\n",
        "model.load_weights(filename)\r\n",
        "with tpu_strategy.scope():\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "\r\n",
        "# pick a random seed\r\n",
        "start = numpy.random.randint(0, len(dataX)-1)\r\n",
        "pattern = dataX[start]\r\n",
        "print(\"Seed:\")\r\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\r\n",
        "\r\n",
        "# generate characters\r\n",
        "for i in range(100):\r\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\r\n",
        "\tx = x / float(n_vocab)\r\n",
        "\tprediction = model.predict(x, verbose=0)\r\n",
        "\tindex = numpy.argmax(prediction)\r\n",
        "\tresult = int_to_char[index]\r\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\r\n",
        "\tsys.stdout.write(result)\r\n",
        "\tpattern.append(index)\r\n",
        "\tpattern = pattern[1:len(pattern)]\r\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" n the open.  a moment later, dyle \n",
            "\tpasses a girl painting a canvas, her easel set up in the\n",
            "\tmiddle \"\n",
            " of the shoulders and she sees the shoulder and the\n",
            "\tsears of the shoulders and she sees the shoulde\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8WB8kbsasz2"
      },
      "source": [
        "#Recurrent Neural Network v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOSl_lApax0x"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "training_doc3 = open('traindata1.txt', encoding='latin-1').read()\r\n",
        "training_doc3 = training_doc3[:100000]\r\n",
        "cleaned = re.sub(r'\\W+', ' ', training_doc3).lower()\r\n",
        "tokens = word_tokenize(cleaned)\r\n",
        "train_len = 4\r\n",
        "text_sequences = []\r\n",
        "\r\n",
        "for i in range(train_len,len(tokens)):\r\n",
        "  seq = tokens[i-train_len:i]\r\n",
        "  text_sequences.append(seq)\r\n",
        "\r\n",
        "sequences = {}\r\n",
        "count = 1\r\n",
        "\r\n",
        "for i in range(len(tokens)):\r\n",
        "  if tokens[i] not in sequences:\r\n",
        "    sequences[tokens[i]] = count\r\n",
        "    count += 1\r\n",
        "\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(text_sequences)\r\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)\r\n",
        "\r\n",
        "#vocabulary size increased by 1 for the cause of padding\r\n",
        "vocabulary_size = len(tokenizer.word_counts)+1\r\n",
        "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\r\n",
        "\r\n",
        "for i in range(len(sequences)):\r\n",
        "  n_sequences[i] = sequences[i]\r\n",
        "\r\n",
        "train_inputs = n_sequences[:,:-1]\r\n",
        "train_targets = n_sequences[:,-1]\r\n",
        "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\r\n",
        "seq_len = train_inputs.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXwYh0b4k3qm",
        "outputId": "6487447e-0276-4ec7-dd75-57c04f8171e2"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Embedding\r\n",
        "\r\n",
        "import os \r\n",
        "import tensorflow as tf\r\n",
        "tf.compat.v1.enable_eager_execution()\r\n",
        "\r\n",
        "try:\r\n",
        "  device_name = os.environ['COLAB_TPU_ADDR']\r\n",
        "  TPU_ADDRESS = 'grpc://' + device_name\r\n",
        "  print('Found TPU at: {}'.format(TPU_ADDRESS))\r\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\r\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\r\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\n",
        "  tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)\r\n",
        "except KeyError:\r\n",
        " print('TPU not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found TPU at: grpc://10.85.8.10:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.85.8.10:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.85.8.10:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y7ZF8TkdOtZ"
      },
      "source": [
        "with tpu_strategy.scope():\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\r\n",
        "  model.add(LSTM(50,return_sequences=True))\r\n",
        "  model.add(LSTM(50))\r\n",
        "  model.add(Dense(50,activation='relu'))\r\n",
        "  model.add(Dense(vocabulary_size, activation='softmax'))\r\n",
        "  # compiling the network\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug_DLsnkk9EF",
        "outputId": "1b14faf2-8bba-44b4-9cc7-d9e5bf446315"
      },
      "source": [
        "with tpu_strategy.scope():\r\n",
        "  model.fit(train_inputs,train_targets,epochs=500,verbose=1)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 3.4603 - accuracy: 0.2346\n",
            "Epoch 2/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.4397 - accuracy: 0.2394\n",
            "Epoch 3/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 3.4133 - accuracy: 0.2419\n",
            "Epoch 4/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 3.3894 - accuracy: 0.2461\n",
            "Epoch 5/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.3715 - accuracy: 0.2495\n",
            "Epoch 6/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.3501 - accuracy: 0.2512\n",
            "Epoch 7/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.3272 - accuracy: 0.2557\n",
            "Epoch 8/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.3100 - accuracy: 0.2611\n",
            "Epoch 9/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.2858 - accuracy: 0.2667\n",
            "Epoch 10/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.2681 - accuracy: 0.2677\n",
            "Epoch 11/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.2502 - accuracy: 0.2710\n",
            "Epoch 12/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.2314 - accuracy: 0.2731\n",
            "Epoch 13/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.2126 - accuracy: 0.2741\n",
            "Epoch 14/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.1969 - accuracy: 0.2781\n",
            "Epoch 15/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.1747 - accuracy: 0.2820\n",
            "Epoch 16/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.1644 - accuracy: 0.2822\n",
            "Epoch 17/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.1409 - accuracy: 0.2884\n",
            "Epoch 18/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.1246 - accuracy: 0.2902\n",
            "Epoch 19/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.1082 - accuracy: 0.2960\n",
            "Epoch 20/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.0966 - accuracy: 0.2946\n",
            "Epoch 21/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.0775 - accuracy: 0.3003\n",
            "Epoch 22/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.0620 - accuracy: 0.2995\n",
            "Epoch 23/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.0498 - accuracy: 0.3002\n",
            "Epoch 24/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.0368 - accuracy: 0.3049\n",
            "Epoch 25/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.0220 - accuracy: 0.3084\n",
            "Epoch 26/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 3.0013 - accuracy: 0.3136\n",
            "Epoch 27/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.9889 - accuracy: 0.3149\n",
            "Epoch 28/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.9775 - accuracy: 0.3165\n",
            "Epoch 29/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.9622 - accuracy: 0.3199\n",
            "Epoch 30/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.9501 - accuracy: 0.3199\n",
            "Epoch 31/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.9356 - accuracy: 0.3239\n",
            "Epoch 32/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.9216 - accuracy: 0.3274\n",
            "Epoch 33/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.9115 - accuracy: 0.3284\n",
            "Epoch 34/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.8964 - accuracy: 0.3329\n",
            "Epoch 35/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.8873 - accuracy: 0.3320\n",
            "Epoch 36/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.8757 - accuracy: 0.3323\n",
            "Epoch 37/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.8594 - accuracy: 0.3383\n",
            "Epoch 38/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.8478 - accuracy: 0.3384\n",
            "Epoch 39/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.8291 - accuracy: 0.3390\n",
            "Epoch 40/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.8275 - accuracy: 0.3407\n",
            "Epoch 41/500\n",
            "553/553 [==============================] - 10s 19ms/step - loss: 2.8210 - accuracy: 0.3437\n",
            "Epoch 42/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.8014 - accuracy: 0.3467\n",
            "Epoch 43/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.7894 - accuracy: 0.3511\n",
            "Epoch 44/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.7891 - accuracy: 0.3480\n",
            "Epoch 45/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.7774 - accuracy: 0.3518\n",
            "Epoch 46/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.7632 - accuracy: 0.3530\n",
            "Epoch 47/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.7550 - accuracy: 0.3564\n",
            "Epoch 48/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.7398 - accuracy: 0.3568\n",
            "Epoch 49/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.7245 - accuracy: 0.3614\n",
            "Epoch 50/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.7182 - accuracy: 0.3636\n",
            "Epoch 51/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.7097 - accuracy: 0.3642\n",
            "Epoch 52/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6971 - accuracy: 0.3675\n",
            "Epoch 53/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6952 - accuracy: 0.3664\n",
            "Epoch 54/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6770 - accuracy: 0.3685\n",
            "Epoch 55/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6695 - accuracy: 0.3716\n",
            "Epoch 56/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6602 - accuracy: 0.3740\n",
            "Epoch 57/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6458 - accuracy: 0.3771\n",
            "Epoch 58/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6438 - accuracy: 0.3772\n",
            "Epoch 59/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.6352 - accuracy: 0.3792\n",
            "Epoch 60/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6265 - accuracy: 0.3809\n",
            "Epoch 61/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6143 - accuracy: 0.3830\n",
            "Epoch 62/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.6076 - accuracy: 0.3812\n",
            "Epoch 63/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.5960 - accuracy: 0.3864\n",
            "Epoch 64/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.5926 - accuracy: 0.3873\n",
            "Epoch 65/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.5777 - accuracy: 0.3933\n",
            "Epoch 66/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.5720 - accuracy: 0.3924\n",
            "Epoch 67/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.5721 - accuracy: 0.3900\n",
            "Epoch 68/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.5590 - accuracy: 0.3951\n",
            "Epoch 69/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.5494 - accuracy: 0.3935\n",
            "Epoch 70/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.5432 - accuracy: 0.3985\n",
            "Epoch 71/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.5276 - accuracy: 0.3977\n",
            "Epoch 72/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 2.5277 - accuracy: 0.3983\n",
            "Epoch 73/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.5186 - accuracy: 0.4044\n",
            "Epoch 74/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.5081 - accuracy: 0.4026\n",
            "Epoch 75/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.4972 - accuracy: 0.4054\n",
            "Epoch 76/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.4984 - accuracy: 0.4033\n",
            "Epoch 77/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.4815 - accuracy: 0.4092\n",
            "Epoch 78/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.4803 - accuracy: 0.4117\n",
            "Epoch 79/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.4642 - accuracy: 0.4117\n",
            "Epoch 80/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.4577 - accuracy: 0.4136\n",
            "Epoch 81/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.4551 - accuracy: 0.4113\n",
            "Epoch 82/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.4505 - accuracy: 0.4164\n",
            "Epoch 83/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.4452 - accuracy: 0.4166\n",
            "Epoch 84/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.4341 - accuracy: 0.4176\n",
            "Epoch 85/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.4306 - accuracy: 0.4192\n",
            "Epoch 86/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.4216 - accuracy: 0.4202\n",
            "Epoch 87/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.4186 - accuracy: 0.4213\n",
            "Epoch 88/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.4088 - accuracy: 0.4213\n",
            "Epoch 89/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3994 - accuracy: 0.4239\n",
            "Epoch 90/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3990 - accuracy: 0.4250\n",
            "Epoch 91/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3812 - accuracy: 0.4282\n",
            "Epoch 92/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3837 - accuracy: 0.4309\n",
            "Epoch 93/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.3790 - accuracy: 0.4293\n",
            "Epoch 94/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.3699 - accuracy: 0.4314\n",
            "Epoch 95/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.3576 - accuracy: 0.4355\n",
            "Epoch 96/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.3521 - accuracy: 0.4371\n",
            "Epoch 97/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3517 - accuracy: 0.4355\n",
            "Epoch 98/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.3484 - accuracy: 0.4337\n",
            "Epoch 99/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.3251 - accuracy: 0.4383\n",
            "Epoch 100/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3312 - accuracy: 0.4390\n",
            "Epoch 101/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3314 - accuracy: 0.4400\n",
            "Epoch 102/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3205 - accuracy: 0.4392\n",
            "Epoch 103/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3103 - accuracy: 0.4444\n",
            "Epoch 104/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3068 - accuracy: 0.4412\n",
            "Epoch 105/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.3011 - accuracy: 0.4454\n",
            "Epoch 106/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.2936 - accuracy: 0.4428\n",
            "Epoch 107/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2903 - accuracy: 0.4493\n",
            "Epoch 108/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2793 - accuracy: 0.4473\n",
            "Epoch 109/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2776 - accuracy: 0.4517\n",
            "Epoch 110/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2695 - accuracy: 0.4537\n",
            "Epoch 111/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2681 - accuracy: 0.4525\n",
            "Epoch 112/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2616 - accuracy: 0.4523\n",
            "Epoch 113/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2478 - accuracy: 0.4545\n",
            "Epoch 114/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.2534 - accuracy: 0.4548\n",
            "Epoch 115/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2407 - accuracy: 0.4570\n",
            "Epoch 116/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.2458 - accuracy: 0.4566\n",
            "Epoch 117/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2304 - accuracy: 0.4611\n",
            "Epoch 118/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.2191 - accuracy: 0.4592\n",
            "Epoch 119/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2216 - accuracy: 0.4613\n",
            "Epoch 120/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2168 - accuracy: 0.4603\n",
            "Epoch 121/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.2047 - accuracy: 0.4647\n",
            "Epoch 122/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.2053 - accuracy: 0.4666\n",
            "Epoch 123/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1950 - accuracy: 0.4647\n",
            "Epoch 124/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1939 - accuracy: 0.4671\n",
            "Epoch 125/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1909 - accuracy: 0.4652\n",
            "Epoch 126/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1753 - accuracy: 0.4726\n",
            "Epoch 127/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1686 - accuracy: 0.4714\n",
            "Epoch 128/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.1784 - accuracy: 0.4680\n",
            "Epoch 129/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1664 - accuracy: 0.4711\n",
            "Epoch 130/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.1767 - accuracy: 0.4706\n",
            "Epoch 131/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.1501 - accuracy: 0.4759\n",
            "Epoch 132/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1405 - accuracy: 0.4775\n",
            "Epoch 133/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1419 - accuracy: 0.4772\n",
            "Epoch 134/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1451 - accuracy: 0.4759\n",
            "Epoch 135/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 2.1318 - accuracy: 0.4816\n",
            "Epoch 136/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1364 - accuracy: 0.4808\n",
            "Epoch 137/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1331 - accuracy: 0.4794\n",
            "Epoch 138/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1126 - accuracy: 0.4848\n",
            "Epoch 139/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 2.1090 - accuracy: 0.4839\n",
            "Epoch 140/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1107 - accuracy: 0.4844\n",
            "Epoch 141/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.1195 - accuracy: 0.4812\n",
            "Epoch 142/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0968 - accuracy: 0.4862\n",
            "Epoch 143/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.0941 - accuracy: 0.4887\n",
            "Epoch 144/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0825 - accuracy: 0.4866\n",
            "Epoch 145/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0811 - accuracy: 0.4892\n",
            "Epoch 146/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0708 - accuracy: 0.4931\n",
            "Epoch 147/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0910 - accuracy: 0.4855\n",
            "Epoch 148/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0739 - accuracy: 0.4931\n",
            "Epoch 149/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0677 - accuracy: 0.4916\n",
            "Epoch 150/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.0738 - accuracy: 0.4904\n",
            "Epoch 151/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0535 - accuracy: 0.4958\n",
            "Epoch 152/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0565 - accuracy: 0.4976\n",
            "Epoch 153/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.0500 - accuracy: 0.4994\n",
            "Epoch 154/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.0348 - accuracy: 0.5006\n",
            "Epoch 155/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0395 - accuracy: 0.4975\n",
            "Epoch 156/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.0323 - accuracy: 0.4993\n",
            "Epoch 157/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0455 - accuracy: 0.4963\n",
            "Epoch 158/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0264 - accuracy: 0.5010\n",
            "Epoch 159/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0247 - accuracy: 0.5040\n",
            "Epoch 160/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0203 - accuracy: 0.5033\n",
            "Epoch 161/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0119 - accuracy: 0.5046\n",
            "Epoch 162/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0195 - accuracy: 0.5054\n",
            "Epoch 163/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 2.0073 - accuracy: 0.5050\n",
            "Epoch 164/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9934 - accuracy: 0.5091\n",
            "Epoch 165/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 2.0021 - accuracy: 0.5076\n",
            "Epoch 166/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9970 - accuracy: 0.5091\n",
            "Epoch 167/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9898 - accuracy: 0.5110\n",
            "Epoch 168/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9925 - accuracy: 0.5089\n",
            "Epoch 169/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 1.9741 - accuracy: 0.5122\n",
            "Epoch 170/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9815 - accuracy: 0.5128\n",
            "Epoch 171/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9755 - accuracy: 0.5101\n",
            "Epoch 172/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9735 - accuracy: 0.5130\n",
            "Epoch 173/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.9714 - accuracy: 0.5144\n",
            "Epoch 174/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9605 - accuracy: 0.5176\n",
            "Epoch 175/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9501 - accuracy: 0.5185\n",
            "Epoch 176/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9503 - accuracy: 0.5162\n",
            "Epoch 177/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9405 - accuracy: 0.5216\n",
            "Epoch 178/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9444 - accuracy: 0.5190\n",
            "Epoch 179/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9366 - accuracy: 0.5181\n",
            "Epoch 180/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9346 - accuracy: 0.5203\n",
            "Epoch 181/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9372 - accuracy: 0.5202\n",
            "Epoch 182/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9217 - accuracy: 0.5218\n",
            "Epoch 183/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9244 - accuracy: 0.5255\n",
            "Epoch 184/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9177 - accuracy: 0.5263\n",
            "Epoch 185/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9211 - accuracy: 0.5230\n",
            "Epoch 186/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9000 - accuracy: 0.5320\n",
            "Epoch 187/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9016 - accuracy: 0.5286\n",
            "Epoch 188/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9021 - accuracy: 0.5286\n",
            "Epoch 189/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.9154 - accuracy: 0.5238\n",
            "Epoch 190/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.9047 - accuracy: 0.5286\n",
            "Epoch 191/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8846 - accuracy: 0.5336\n",
            "Epoch 192/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8976 - accuracy: 0.5297\n",
            "Epoch 193/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8920 - accuracy: 0.5299\n",
            "Epoch 194/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8797 - accuracy: 0.5327\n",
            "Epoch 195/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8773 - accuracy: 0.5342\n",
            "Epoch 196/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 1.8692 - accuracy: 0.5361\n",
            "Epoch 197/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 1.8686 - accuracy: 0.5378\n",
            "Epoch 198/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 1.8614 - accuracy: 0.5383\n",
            "Epoch 199/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8651 - accuracy: 0.5390\n",
            "Epoch 200/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8638 - accuracy: 0.5357\n",
            "Epoch 201/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8669 - accuracy: 0.5385\n",
            "Epoch 202/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8488 - accuracy: 0.5400\n",
            "Epoch 203/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.8411 - accuracy: 0.5404\n",
            "Epoch 204/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8408 - accuracy: 0.5422\n",
            "Epoch 205/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8411 - accuracy: 0.5435\n",
            "Epoch 206/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.8422 - accuracy: 0.5424\n",
            "Epoch 207/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8488 - accuracy: 0.5390\n",
            "Epoch 208/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8410 - accuracy: 0.5438\n",
            "Epoch 209/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8317 - accuracy: 0.5462\n",
            "Epoch 210/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8203 - accuracy: 0.5471\n",
            "Epoch 211/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8309 - accuracy: 0.5429\n",
            "Epoch 212/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8189 - accuracy: 0.5474\n",
            "Epoch 213/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.8223 - accuracy: 0.5438\n",
            "Epoch 214/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8057 - accuracy: 0.5520\n",
            "Epoch 215/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8103 - accuracy: 0.5486\n",
            "Epoch 216/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7947 - accuracy: 0.5531\n",
            "Epoch 217/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7963 - accuracy: 0.5538\n",
            "Epoch 218/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.8050 - accuracy: 0.5482\n",
            "Epoch 219/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7919 - accuracy: 0.5523\n",
            "Epoch 220/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7913 - accuracy: 0.5512\n",
            "Epoch 221/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7887 - accuracy: 0.5525\n",
            "Epoch 222/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7929 - accuracy: 0.5504\n",
            "Epoch 223/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7823 - accuracy: 0.5545\n",
            "Epoch 224/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7809 - accuracy: 0.5561\n",
            "Epoch 225/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7737 - accuracy: 0.5589\n",
            "Epoch 226/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7705 - accuracy: 0.5555\n",
            "Epoch 227/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7666 - accuracy: 0.5608\n",
            "Epoch 228/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7780 - accuracy: 0.5544\n",
            "Epoch 229/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7616 - accuracy: 0.5588\n",
            "Epoch 230/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7484 - accuracy: 0.5597\n",
            "Epoch 231/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7543 - accuracy: 0.5606\n",
            "Epoch 232/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7494 - accuracy: 0.5622\n",
            "Epoch 233/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.7451 - accuracy: 0.5641\n",
            "Epoch 234/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.7470 - accuracy: 0.5576\n",
            "Epoch 235/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7461 - accuracy: 0.5596\n",
            "Epoch 236/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7447 - accuracy: 0.5607\n",
            "Epoch 237/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.7291 - accuracy: 0.5649\n",
            "Epoch 238/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7375 - accuracy: 0.5612\n",
            "Epoch 239/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.7292 - accuracy: 0.5666\n",
            "Epoch 240/500\n",
            "553/553 [==============================] - 9s 16ms/step - loss: 1.7248 - accuracy: 0.5608\n",
            "Epoch 241/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7244 - accuracy: 0.5664\n",
            "Epoch 242/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7299 - accuracy: 0.5615\n",
            "Epoch 243/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7323 - accuracy: 0.5660\n",
            "Epoch 244/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7052 - accuracy: 0.5722\n",
            "Epoch 245/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.7096 - accuracy: 0.5710\n",
            "Epoch 246/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.7105 - accuracy: 0.5725\n",
            "Epoch 247/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.6939 - accuracy: 0.5751\n",
            "Epoch 248/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7074 - accuracy: 0.5711\n",
            "Epoch 249/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.7090 - accuracy: 0.5685\n",
            "Epoch 250/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.7049 - accuracy: 0.5701\n",
            "Epoch 251/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6873 - accuracy: 0.5732\n",
            "Epoch 252/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6824 - accuracy: 0.5778\n",
            "Epoch 253/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.6925 - accuracy: 0.5725\n",
            "Epoch 254/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.6832 - accuracy: 0.5746\n",
            "Epoch 255/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.6769 - accuracy: 0.5786\n",
            "Epoch 256/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.6767 - accuracy: 0.5745\n",
            "Epoch 257/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6743 - accuracy: 0.5762\n",
            "Epoch 258/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6827 - accuracy: 0.5749\n",
            "Epoch 259/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6745 - accuracy: 0.5750\n",
            "Epoch 260/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6604 - accuracy: 0.5778\n",
            "Epoch 261/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6574 - accuracy: 0.5800\n",
            "Epoch 262/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6689 - accuracy: 0.5785\n",
            "Epoch 263/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6701 - accuracy: 0.5808\n",
            "Epoch 264/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.6595 - accuracy: 0.5814\n",
            "Epoch 265/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.6555 - accuracy: 0.5793\n",
            "Epoch 266/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6432 - accuracy: 0.5821\n",
            "Epoch 267/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6447 - accuracy: 0.5847\n",
            "Epoch 268/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6408 - accuracy: 0.5860\n",
            "Epoch 269/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6361 - accuracy: 0.5869\n",
            "Epoch 270/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6344 - accuracy: 0.5833\n",
            "Epoch 271/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.6395 - accuracy: 0.5881\n",
            "Epoch 272/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.6372 - accuracy: 0.5847\n",
            "Epoch 273/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6271 - accuracy: 0.5882\n",
            "Epoch 274/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6239 - accuracy: 0.5892\n",
            "Epoch 275/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6270 - accuracy: 0.5874\n",
            "Epoch 276/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.6164 - accuracy: 0.5901\n",
            "Epoch 277/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6241 - accuracy: 0.5860\n",
            "Epoch 278/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6092 - accuracy: 0.5934\n",
            "Epoch 279/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6051 - accuracy: 0.5900\n",
            "Epoch 280/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6137 - accuracy: 0.5920\n",
            "Epoch 281/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6167 - accuracy: 0.5905\n",
            "Epoch 282/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6049 - accuracy: 0.5939\n",
            "Epoch 283/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5966 - accuracy: 0.5910\n",
            "Epoch 284/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6042 - accuracy: 0.5899\n",
            "Epoch 285/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.6032 - accuracy: 0.5926\n",
            "Epoch 286/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5875 - accuracy: 0.5959\n",
            "Epoch 287/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5933 - accuracy: 0.5941\n",
            "Epoch 288/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5997 - accuracy: 0.5939\n",
            "Epoch 289/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5969 - accuracy: 0.5921\n",
            "Epoch 290/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5856 - accuracy: 0.5953\n",
            "Epoch 291/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5763 - accuracy: 0.6014\n",
            "Epoch 292/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5835 - accuracy: 0.5972\n",
            "Epoch 293/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5871 - accuracy: 0.5956\n",
            "Epoch 294/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5589 - accuracy: 0.6037\n",
            "Epoch 295/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5708 - accuracy: 0.5995\n",
            "Epoch 296/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5708 - accuracy: 0.5999\n",
            "Epoch 297/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5681 - accuracy: 0.6002\n",
            "Epoch 298/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5694 - accuracy: 0.6014\n",
            "Epoch 299/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5497 - accuracy: 0.6011\n",
            "Epoch 300/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5472 - accuracy: 0.6063\n",
            "Epoch 301/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5465 - accuracy: 0.6071\n",
            "Epoch 302/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5569 - accuracy: 0.6026\n",
            "Epoch 303/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5645 - accuracy: 0.6038\n",
            "Epoch 304/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.5466 - accuracy: 0.6094\n",
            "Epoch 305/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5461 - accuracy: 0.6052\n",
            "Epoch 306/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5476 - accuracy: 0.6038\n",
            "Epoch 307/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5412 - accuracy: 0.6087\n",
            "Epoch 308/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5397 - accuracy: 0.6083\n",
            "Epoch 309/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5346 - accuracy: 0.6079\n",
            "Epoch 310/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5342 - accuracy: 0.6083\n",
            "Epoch 311/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5209 - accuracy: 0.6125\n",
            "Epoch 312/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5300 - accuracy: 0.6066\n",
            "Epoch 313/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5383 - accuracy: 0.6092\n",
            "Epoch 314/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5229 - accuracy: 0.6118\n",
            "Epoch 315/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5129 - accuracy: 0.6148\n",
            "Epoch 316/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5242 - accuracy: 0.6115\n",
            "Epoch 317/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5322 - accuracy: 0.6078\n",
            "Epoch 318/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5095 - accuracy: 0.6154\n",
            "Epoch 319/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5112 - accuracy: 0.6149\n",
            "Epoch 320/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5032 - accuracy: 0.6161\n",
            "Epoch 321/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.5092 - accuracy: 0.6144\n",
            "Epoch 322/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.5095 - accuracy: 0.6114\n",
            "Epoch 323/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5065 - accuracy: 0.6169\n",
            "Epoch 324/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.5018 - accuracy: 0.6154\n",
            "Epoch 325/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.5023 - accuracy: 0.6157\n",
            "Epoch 326/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.4997 - accuracy: 0.6174\n",
            "Epoch 327/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4864 - accuracy: 0.6197\n",
            "Epoch 328/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4847 - accuracy: 0.6213\n",
            "Epoch 329/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4839 - accuracy: 0.6194\n",
            "Epoch 330/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4853 - accuracy: 0.6192\n",
            "Epoch 331/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4918 - accuracy: 0.6193\n",
            "Epoch 332/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4863 - accuracy: 0.6186\n",
            "Epoch 333/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.4682 - accuracy: 0.6268\n",
            "Epoch 334/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4719 - accuracy: 0.6230\n",
            "Epoch 335/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4724 - accuracy: 0.6229\n",
            "Epoch 336/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4764 - accuracy: 0.6206\n",
            "Epoch 337/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4672 - accuracy: 0.6239\n",
            "Epoch 338/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4851 - accuracy: 0.6189\n",
            "Epoch 339/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.4709 - accuracy: 0.6206\n",
            "Epoch 340/500\n",
            "553/553 [==============================] - 9s 17ms/step - loss: 1.4532 - accuracy: 0.6246\n",
            "Epoch 341/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4650 - accuracy: 0.6255\n",
            "Epoch 342/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4654 - accuracy: 0.6238\n",
            "Epoch 343/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4553 - accuracy: 0.6256\n",
            "Epoch 344/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4479 - accuracy: 0.6298\n",
            "Epoch 345/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4458 - accuracy: 0.6308\n",
            "Epoch 346/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4412 - accuracy: 0.6283\n",
            "Epoch 347/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4399 - accuracy: 0.6285\n",
            "Epoch 348/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4505 - accuracy: 0.6260\n",
            "Epoch 349/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4445 - accuracy: 0.6263\n",
            "Epoch 350/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4447 - accuracy: 0.6271\n",
            "Epoch 351/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4436 - accuracy: 0.6314\n",
            "Epoch 352/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4276 - accuracy: 0.6317\n",
            "Epoch 353/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4311 - accuracy: 0.6320\n",
            "Epoch 354/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4480 - accuracy: 0.6257\n",
            "Epoch 355/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4304 - accuracy: 0.6359\n",
            "Epoch 356/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4327 - accuracy: 0.6324\n",
            "Epoch 357/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.4254 - accuracy: 0.6363\n",
            "Epoch 358/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4191 - accuracy: 0.6359\n",
            "Epoch 359/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4209 - accuracy: 0.6333\n",
            "Epoch 360/500\n",
            "553/553 [==============================] - 10s 19ms/step - loss: 1.4170 - accuracy: 0.6346\n",
            "Epoch 361/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4109 - accuracy: 0.6359\n",
            "Epoch 362/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4367 - accuracy: 0.6261\n",
            "Epoch 363/500\n",
            "553/553 [==============================] - 10s 19ms/step - loss: 1.4186 - accuracy: 0.6328\n",
            "Epoch 364/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4208 - accuracy: 0.6317\n",
            "Epoch 365/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4035 - accuracy: 0.6379\n",
            "Epoch 366/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3984 - accuracy: 0.6374\n",
            "Epoch 367/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3954 - accuracy: 0.6395\n",
            "Epoch 368/500\n",
            "553/553 [==============================] - 11s 19ms/step - loss: 1.4132 - accuracy: 0.6342\n",
            "Epoch 369/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3968 - accuracy: 0.6390\n",
            "Epoch 370/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3924 - accuracy: 0.6399\n",
            "Epoch 371/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3903 - accuracy: 0.6416\n",
            "Epoch 372/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4104 - accuracy: 0.6375\n",
            "Epoch 373/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.4081 - accuracy: 0.6364\n",
            "Epoch 374/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3871 - accuracy: 0.6406\n",
            "Epoch 375/500\n",
            "553/553 [==============================] - 10s 19ms/step - loss: 1.3769 - accuracy: 0.6419\n",
            "Epoch 376/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3843 - accuracy: 0.6415\n",
            "Epoch 377/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3856 - accuracy: 0.6404\n",
            "Epoch 378/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3833 - accuracy: 0.6396\n",
            "Epoch 379/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3695 - accuracy: 0.6476\n",
            "Epoch 380/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3803 - accuracy: 0.6422\n",
            "Epoch 381/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3829 - accuracy: 0.6418\n",
            "Epoch 382/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3742 - accuracy: 0.6463\n",
            "Epoch 383/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3749 - accuracy: 0.6457\n",
            "Epoch 384/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3869 - accuracy: 0.6426\n",
            "Epoch 385/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3451 - accuracy: 0.6507\n",
            "Epoch 386/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3572 - accuracy: 0.6525\n",
            "Epoch 387/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3555 - accuracy: 0.6475\n",
            "Epoch 388/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3684 - accuracy: 0.6487\n",
            "Epoch 389/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3718 - accuracy: 0.6445\n",
            "Epoch 390/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3494 - accuracy: 0.6510\n",
            "Epoch 391/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3487 - accuracy: 0.6491\n",
            "Epoch 392/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3527 - accuracy: 0.6468\n",
            "Epoch 393/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3678 - accuracy: 0.6447\n",
            "Epoch 394/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3527 - accuracy: 0.6469\n",
            "Epoch 395/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3463 - accuracy: 0.6521\n",
            "Epoch 396/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3313 - accuracy: 0.6563\n",
            "Epoch 397/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3435 - accuracy: 0.6523\n",
            "Epoch 398/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3415 - accuracy: 0.6556\n",
            "Epoch 399/500\n",
            "553/553 [==============================] - 11s 19ms/step - loss: 1.3413 - accuracy: 0.6501\n",
            "Epoch 400/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3537 - accuracy: 0.6500\n",
            "Epoch 401/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3382 - accuracy: 0.6530\n",
            "Epoch 402/500\n",
            "553/553 [==============================] - 10s 19ms/step - loss: 1.3312 - accuracy: 0.6551\n",
            "Epoch 403/500\n",
            "553/553 [==============================] - 10s 19ms/step - loss: 1.3129 - accuracy: 0.6598\n",
            "Epoch 404/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3368 - accuracy: 0.6529\n",
            "Epoch 405/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3425 - accuracy: 0.6513\n",
            "Epoch 406/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3175 - accuracy: 0.6590\n",
            "Epoch 407/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3291 - accuracy: 0.6547\n",
            "Epoch 408/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3259 - accuracy: 0.6558\n",
            "Epoch 409/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3231 - accuracy: 0.6561\n",
            "Epoch 410/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3070 - accuracy: 0.6600\n",
            "Epoch 411/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3086 - accuracy: 0.6608\n",
            "Epoch 412/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3223 - accuracy: 0.6566\n",
            "Epoch 413/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3073 - accuracy: 0.6599\n",
            "Epoch 414/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3262 - accuracy: 0.6552\n",
            "Epoch 415/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3084 - accuracy: 0.6564\n",
            "Epoch 416/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2939 - accuracy: 0.6618\n",
            "Epoch 417/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2925 - accuracy: 0.6627\n",
            "Epoch 418/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3118 - accuracy: 0.6603\n",
            "Epoch 419/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3173 - accuracy: 0.6574\n",
            "Epoch 420/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.3124 - accuracy: 0.6573\n",
            "Epoch 421/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2991 - accuracy: 0.6608\n",
            "Epoch 422/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2948 - accuracy: 0.6615\n",
            "Epoch 423/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2958 - accuracy: 0.6610\n",
            "Epoch 424/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2883 - accuracy: 0.6619\n",
            "Epoch 425/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2867 - accuracy: 0.6647\n",
            "Epoch 426/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2802 - accuracy: 0.6673\n",
            "Epoch 427/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2930 - accuracy: 0.6631\n",
            "Epoch 428/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2838 - accuracy: 0.6650\n",
            "Epoch 429/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2807 - accuracy: 0.6638\n",
            "Epoch 430/500\n",
            "553/553 [==============================] - 10s 19ms/step - loss: 1.2761 - accuracy: 0.6653\n",
            "Epoch 431/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2819 - accuracy: 0.6638\n",
            "Epoch 432/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2874 - accuracy: 0.6634\n",
            "Epoch 433/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2803 - accuracy: 0.6653\n",
            "Epoch 434/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2687 - accuracy: 0.6674\n",
            "Epoch 435/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2706 - accuracy: 0.6653\n",
            "Epoch 436/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2775 - accuracy: 0.6660\n",
            "Epoch 437/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2659 - accuracy: 0.6676\n",
            "Epoch 438/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2686 - accuracy: 0.6658\n",
            "Epoch 439/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2469 - accuracy: 0.6739\n",
            "Epoch 440/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2855 - accuracy: 0.6639\n",
            "Epoch 441/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2619 - accuracy: 0.6689\n",
            "Epoch 442/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2723 - accuracy: 0.6665\n",
            "Epoch 443/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2569 - accuracy: 0.6724\n",
            "Epoch 444/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2457 - accuracy: 0.6722\n",
            "Epoch 445/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2452 - accuracy: 0.6719\n",
            "Epoch 446/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2377 - accuracy: 0.6756\n",
            "Epoch 447/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2455 - accuracy: 0.6721\n",
            "Epoch 448/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2633 - accuracy: 0.6668\n",
            "Epoch 449/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2614 - accuracy: 0.6676\n",
            "Epoch 450/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2364 - accuracy: 0.6758\n",
            "Epoch 451/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2428 - accuracy: 0.6738\n",
            "Epoch 452/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2500 - accuracy: 0.6720\n",
            "Epoch 453/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2453 - accuracy: 0.6726\n",
            "Epoch 454/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.2449 - accuracy: 0.6745\n",
            "Epoch 455/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2291 - accuracy: 0.6773\n",
            "Epoch 456/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2286 - accuracy: 0.6804\n",
            "Epoch 457/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2322 - accuracy: 0.6735\n",
            "Epoch 458/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2464 - accuracy: 0.6726\n",
            "Epoch 459/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2100 - accuracy: 0.6838\n",
            "Epoch 460/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.2250 - accuracy: 0.6776\n",
            "Epoch 461/500\n",
            "553/553 [==============================] - 10s 19ms/step - loss: 1.2197 - accuracy: 0.6797\n",
            "Epoch 462/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2207 - accuracy: 0.6809\n",
            "Epoch 463/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2328 - accuracy: 0.6736\n",
            "Epoch 464/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2435 - accuracy: 0.6714\n",
            "Epoch 465/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2241 - accuracy: 0.6780\n",
            "Epoch 466/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2159 - accuracy: 0.6787\n",
            "Epoch 467/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2003 - accuracy: 0.6848\n",
            "Epoch 468/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2185 - accuracy: 0.6792\n",
            "Epoch 469/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2180 - accuracy: 0.6765\n",
            "Epoch 470/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.2030 - accuracy: 0.6834\n",
            "Epoch 471/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2178 - accuracy: 0.6774\n",
            "Epoch 472/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2069 - accuracy: 0.6878\n",
            "Epoch 473/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2066 - accuracy: 0.6804\n",
            "Epoch 474/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2023 - accuracy: 0.6817\n",
            "Epoch 475/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2047 - accuracy: 0.6842\n",
            "Epoch 476/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1997 - accuracy: 0.6853\n",
            "Epoch 477/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1916 - accuracy: 0.6856\n",
            "Epoch 478/500\n",
            "553/553 [==============================] - 10s 17ms/step - loss: 1.1882 - accuracy: 0.6879\n",
            "Epoch 479/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1835 - accuracy: 0.6879\n",
            "Epoch 480/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2131 - accuracy: 0.6810\n",
            "Epoch 481/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1862 - accuracy: 0.6866\n",
            "Epoch 482/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2041 - accuracy: 0.6802\n",
            "Epoch 483/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.2054 - accuracy: 0.6827\n",
            "Epoch 484/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1915 - accuracy: 0.6862\n",
            "Epoch 485/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1845 - accuracy: 0.6859\n",
            "Epoch 486/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1790 - accuracy: 0.6891\n",
            "Epoch 487/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1901 - accuracy: 0.6859\n",
            "Epoch 488/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1847 - accuracy: 0.6881\n",
            "Epoch 489/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1837 - accuracy: 0.6861\n",
            "Epoch 490/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1655 - accuracy: 0.6929\n",
            "Epoch 491/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1685 - accuracy: 0.6926\n",
            "Epoch 492/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1706 - accuracy: 0.6919\n",
            "Epoch 493/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1804 - accuracy: 0.6890\n",
            "Epoch 494/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1855 - accuracy: 0.6878\n",
            "Epoch 495/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1575 - accuracy: 0.6935\n",
            "Epoch 496/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1791 - accuracy: 0.6898\n",
            "Epoch 497/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1631 - accuracy: 0.6939\n",
            "Epoch 498/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1644 - accuracy: 0.6925\n",
            "Epoch 499/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1619 - accuracy: 0.6918\n",
            "Epoch 500/500\n",
            "553/553 [==============================] - 10s 18ms/step - loss: 1.1543 - accuracy: 0.6977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "UtSaucGCdXO1",
        "outputId": "959c0e5a-7513-4cc4-8532-1416a61a8f99"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "input_text = input().strip().lower()\r\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\r\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\r\n",
        "print(encoded_text, pad_encoded)\r\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\r\n",
        "  pred_word = tokenizer.index_word[i]\r\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello how are\n",
            "[941, 125, 29] [[941 125  29]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-16619530897f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpad_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mpred_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Next word suggestion:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:279 run\n        return self.extended.tpu_run(fn, args, kwargs, options)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1296 tpu_run\n        return func(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1364 tpu_function\n        xla_options=tpu.XLAOptions(use_spmd_for_xla_partitioning=False))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:968 replicate\n        xla_options=xla_options)[1]\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:1439 split_compile_and_replicate\n        outputs = computation(*computation_inputs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1325 replicated_fn\n        result[0] = fn(*replica_args, **replica_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer sequential_1 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 3)\n"
          ]
        }
      ]
    }
  ]
}